path:
  train_path: ../data/train.csv
  test_path: ../data/dev.csv
  predict_path: ../data/test.csv
  save_path: save_models/

data:
  shuffle: True
  train_ratio: 0.8
  swap: True

model:
  model_name: xlm-roberta-large

train:
  max_epoch: 40
  batch_size: 64
  learning_rate: 0.0000056
  loss: mse
  use_frozen: False
  
utils:
  seed: 42
  monitor: val_pearson
  patience: 25
  top_k: 1

k_fold:
  use_k_fold: True
  num_folds: 5
  num_split: 5
  
wandb:
  project: nlp-08-level1-sts