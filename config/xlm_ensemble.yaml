path:
  train_path: ../data/train.csv
  test_path: ../data/dev.csv
  predict_path: ../data/test.csv
  save_path: save_models/

data:
  shuffle: True
  train_ratio: 0.8
  swap: True

model:
  model_name: xlm-roberta-large

train:
  max_epoch: 33
  batch_size: 32
  learning_rate: 56e-6
  loss: mse
  use_frozen: False
  
utils:
  seed: 42
  monitor: val_pearson
  patience: 25
  top_k: 3

k_fold:
  use_k_fold: False
  num_folds: 3
  num_split: 5
  
wandb:
  project: nlp-08-level1-sts